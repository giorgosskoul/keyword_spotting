{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825fe381",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates the process of training a new openWakeWord model, using synthetic speech generated with open-source TTS models, and negative data representing music, noise, and speech. While the process here is complete, only small samples of datasets are utilized so that a new model can be trained on CPUs. In practice, much larger volumes of data (both positive and negitive examples) is needed to produce robust models. See the [documentation](https://github.com/dscripka/openWakeWord/tree/main/docs/models) for the pre-trained openWakeWord models for more information about how these models were trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e4597",
   "metadata": {},
   "source": [
    "To start, we'll need to install the requirements needed to train new openWakeWord models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba07a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements (it's recommended that you do this in a new virtual environment)\n",
    "\n",
    "# !pip install openwakeword\n",
    "# !pip install speechbrain\n",
    "# !pip install datasets\n",
    "# !pip install scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c914b0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T03:26:26.308309Z",
     "start_time": "2023-02-18T03:26:24.785801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from numpy.lib.format import open_memmap\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import openwakeword\n",
    "import openwakeword.data\n",
    "import openwakeword.utils\n",
    "import openwakeword.metrics\n",
    "\n",
    "import scipy\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a7f25",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee94c6e",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea736a",
   "metadata": {},
   "source": [
    "Next we'll load the data used for training. For the purposes of this demonstration, we'll use a small set of positive and negative.\n",
    "\n",
    "For the positive data, there are ~3400 synthetic examples of the phrase \"turn on the office lights\" that were produced with the text-to-speech models documented in a [separate repo](https://github.com/dscripka/synthetic_speech_dataset_generation).\n",
    "\n",
    "These positive examples can be downloaded [here](https://f002.backblazeb2.com/file/openwakeword-resources/data/turn_on_the_office_lights.tar.gz).\n",
    "\n",
    "For negative data, we'll use small, already prepared samples of the [fma-large dataset](https://github.com/mdeff/fma) for music, the [FSD50k dataset](https://zenodo.org/record/4060432#.Y-hA2BzMJhE) for noise, and the [Common Voice 11](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset for speech.\n",
    "\n",
    "The fma-large sample can be downloaded [here](https://f002.backblazeb2.com/file/openwakeword-resources/data/fma_sample.zip), and then extracted into the working director.\n",
    "\n",
    "The FSD50k sample can be downloaded [here](https://f002.backblazeb2.com/file/openwakeword-resources/data/fsd50k_sample.zip), and then extracted into the working directory.\n",
    "\n",
    "And we'll use the HuggingFace Datasets library to get a portion of the test split of the Common Voice 11 (CV11) corpus.\n",
    "\n",
    "Note the data provided here is intended for non-commerical applications only; you will need to verify the license status of this (and other) data if you intend to use it for commerical purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a31f760c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T16:08:32.045657Z",
     "start_time": "2023-02-12T16:07:24.104475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16354it [00:00, 26183.62it/s]\n",
      "100%|██████████| 5000/5000 [00:44<00:00, 112.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download CV11 test split from HuggingFace, and convert the audio into 16 khz, 16-bit wav files\n",
    "\n",
    "cv_11 = datasets.load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n",
    "cv_11 = cv_11.cast_column(\"audio\", datasets.Audio(sampling_rate=16000, mono=True)) # convert to 16-khz\n",
    "cv_11 = iter(cv_11)\n",
    "\n",
    "# Convert and save clips (only first 5000)\n",
    "limit = 5000\n",
    "for i in tqdm(range(limit)):\n",
    "    example = next(cv_11)\n",
    "    output = os.path.join(\"cv11_test_clips\", example[\"path\"][0:-4] + \".wav\")\n",
    "    os.makedirs(os.path.dirname(output), exist_ok=True)\n",
    "\n",
    "    wav_data = (example[\"audio\"][\"array\"]*32767).astype(np.int16) # convert to 16-bit PCM format\n",
    "    scipy.io.wavfile.write(output, 16000, wav_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
